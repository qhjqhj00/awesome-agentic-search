import streamlit as st
st.set_page_config(page_title="ğŸ§  Agentic Chat Demo", page_icon="ğŸ§ ", layout="wide")


from agent.api_agent import Agent
import json

from utils import *
import copy
with open("config/api_config.json", "r") as f:
    api_config = json.load(f)

# Streamlit é¡µé¢è®¾ç½®

st.title("ğŸ§  Agentic Chat Demo")
set_chat_message_style()
use_generator, generator, generator_name, use_retriever, retriever, retriever_name = show_options()

if "messages_frontend" not in st.session_state:
    st.session_state.messages_frontend = []
if "messages_backend" not in st.session_state:
    st.session_state.messages_backend = []

display_chat_messages()
# Add clear history button to sidebar
with st.sidebar:
    if st.button("Clear History"):
        st.session_state.messages_frontend = []
        st.session_state.messages_backend = []
        st.rerun()

mode, mode_message = mode_check(generator_name, retriever_name)
if not mode:
    st.error(mode_message)
    st.stop()

file_path = show_file_uploader()


if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
   
    with st.chat_message("user"):
        if file_path:
            file_type = file_type_check(file_path)
            show_file(file_path, file_type.split("_")[0])
        st.markdown(prompt)
    

    frontend_history_manager(st.session_state.messages_frontend, prompt, generator_name, file_path)
    # é€‰æ‹©è°ƒç”¨å¯¹è±¡
    if use_generator and use_retriever:
        # é¢„ç•™ rag_pipeline
        with st.chat_message("assistant"):
            st.markdown("_RAG Pipeline æš‚æœªå®ç°_")
            raise ValueError("RAG Pipeline æš‚æœªå®ç°")
    elif use_generator:
        # ä½¿ç”¨ generator agent æµå¼è¾“å‡º
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            response = ""
            # æ„é€ å†å²æ¶ˆæ¯ç”¨äºå¤šè½®å¯¹è¯
            backend_history_manager(st.session_state.messages_backend, generator_name, prompt, mode, file_path)
            # æµå¼è¾“å‡º
            for chunk in generator.stream_completion(st.session_state.messages_backend):
                response += chunk
                response_placeholder.markdown(response + "â–Œ")
            response_placeholder.markdown(response)
            st.session_state.messages_frontend.append({"role": "assistant", "content": response})
            st.session_state.messages_backend.append({"role": "assistant", "content": response})
            st.rerun()
    else:
        with st.chat_message("assistant"):
            st.markdown("_è¯·è‡³å°‘é€‰æ‹© Generator_")
            raise ValueError("è¯·è‡³å°‘é€‰æ‹© Generator")


