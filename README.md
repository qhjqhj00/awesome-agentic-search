# <div align="center">ğŸ” Awesome Agentic Search</div>
<h4 align="center">
<p>
<a href="#-objectives">ğŸ¯ Objectives</a> |
<a href="#-papers">ğŸ“š Papers</a> |
<a href="#-slides">ğŸ“Š Slides</a> |
<a href="#-demo">ğŸ® Demo</a> |
<a href="#-arena">ğŸ† Arena</a> |
<a href="#-gym">ğŸ‹ï¸ Gym</a>
</p>
</h4>
ğŸ¤– Agentic search is an advanced AI approach where autonomous agents actively plan and execute multi-step, iterative searches to decompose complex queries, evaluate relevance, and synthesize responsesâ€”transforming them from passive retrievers into dynamic, reasoning-driven researchers.

## ğŸ¯ Objectives
> ğŸš§ **Note**: This project is evolving rapidlyâ€”join the community by opening issues, submitting PRs, leaving comments, or  â­ starring the repo to help build a leading resource for agentic search.

- **Research Collection**: Curate and categorize comprehensive research work in agentic search, including papers, code implementations, and empirical findings

- **Interactive Demos**: Build demonstration pages to showcase different agentic search methods and allow hands-on exploration of their capabilities

- **Evaluation Arena**: Develop a Python toolkit for systematic evaluation and benchmarking of agentic search methods across diverse tasks and metrics

- **Training Gym**: Create a Python framework for training and optimizing agentic search models, including reinforcement learning and other approaches


## ğŸ“š Papers
For each paper, we provide the following information:
 > ğŸ‘¨â€ğŸ“ **First Author** Â· ğŸ“§ **Corresponding Author (Last Author if not specified)** Â· ğŸ›ï¸ **First Organization** Â· ğŸ“Š **Dataset** 
 
  *Note: Please submit a PR if we missed anything!*

ğŸ“Š Dataset Types:

**General QA**: NQ, TriviaQA, PopQA

**Multi-Hop QA**: HotpotQA, 2wiki, Musique, Bamboogle

**Complex Task**: GPQA, GAIA, WebWalker QA, Humanity's Last Exam (HLE)

**Report Generation**: Glaive

**Math & Coding**: AIME, MATH500, AMC, LiveCodeBench

### ğŸ“ Training-based
[Search-R1: Training LLMs to Reason and Leverage Search
Engines with Reinforcement Learning](https://arxiv.org/abs/2503.09516) ![GitHub Repo stars](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1?style=social)

 > ğŸ‘¨â€ğŸ“ **Bowen Jin** Â· ğŸ“§ **Jiawei Han** Â· ğŸ›ï¸ **UIUC** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Qwen-2.5-3B / 7B Â· ğŸ¯ **Training**: GRPO, PPO

[An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents](https://arxiv.org/pdf/2505.15117) ![GitHub Repo stars](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1?style=social)

 > ğŸ‘¨â€ğŸ“ **Bowen Jin** Â· ğŸ“§ **Jiawei Han** Â· ğŸ›ï¸ **UIUC** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Qwen-2.5-3B / 7B /  14BÂ· ğŸ¯ **Training**: GRPO, PPO
 
 *Notes: a new version of Search-R1.*

[WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776):![GitHub Repo stars](https://img.shields.io/github/stars/RUC-NLPIR/WebThinker?style=social)

 > ğŸ‘¨â€ğŸ“ **Xiaoxi Li** Â· ğŸ“§ **Zhicheng Dou** Â· ğŸ›ï¸ **GSAI, RUC** \
 > ğŸ“Š **Dataset**: Complex Task, Report Generation Â· ğŸ¤– **Model**: QwQ 32B Â· ğŸ¯ **Training**: SFT, DPO


[DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments](https://arxiv.org/abs/2504.03160) [![[code]](https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher)](https://github.com/GAIR-NLP/DeepResearcher)

 > ğŸ‘¨â€ğŸ“ **Yuxiang Zheng** Â· ğŸ“§ **Pengfei Liu** Â· ğŸ›ï¸ **SJTU** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Qwen-2.5-7B Â· ğŸ¯ **Training**: GRPO


[R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2503.05592) [![[code]](https://img.shields.io/github/stars/RUCAIBox/R1-Searcher)](https://github.com/RUCAIBox/R1-Searcher)

 > ğŸ‘¨â€ğŸ“ **Huatong Song** Â· ğŸ“§ **Wayne Xin Zhao** Â· ğŸ›ï¸ **GSAI, RUC** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Qwen-2.5-7B, Llama-3.1-8B Â· ğŸ¯ **Training**: SFT, GRPO, REINFORCE++

 [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.17005) [![[code]](https://img.shields.io/github/stars/RUCAIBox/R1-Searcher-plus)](https://github.com/RUCAIBox/R1-Searcher-plus)

  > ğŸ‘¨â€ğŸ“ **Huatong Song** Â· ğŸ“§ **Wayne Xin Zhao** Â· ğŸ›ï¸ **GSAI, RUC** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Qwen-2.5-7B Â· ğŸ¯ **Training**: SFT, GRPO, REINFORCE++

[SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834) [![[code]](https://img.shields.io/github/stars/RUCAIBox/SimpleDeepSearcher)](https://github.com/RUCAIBox/SimpleDeepSearcher)

  > ğŸ‘¨â€ğŸ“ **Shuang Sun** Â· ğŸ“§ **Wayne Xin Zhao** Â· ğŸ›ï¸ **GSAI, RUC** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Qwen-2.5-7B / 32B, QwQ-32B Â· ğŸ¯ **Training**: SFT, DPO, REINFORCE++

[ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588) [![[code]](https://img.shields.io/github/stars/Alibaba-NLP/ZeroSearch)](https://github.com/Alibaba-NLP/ZeroSearch)

 > ğŸ‘¨â€ğŸ“ **Hao Sun** Â· ğŸ“§ **Zile Qiao, Jiayan Guo, Yan Zhang** Â· ğŸ›ï¸ **Tongyi Lab** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Qwen-2.5-3B / 7B, s LLaMA-3.2-3B Â· ğŸ¯ **Training**: REINFORCE, GRPO, PPO

[Chain-of-Retrieval Augmented Generation](https://arxiv.org/abs/2501.14342) [![[code]](https://img.shields.io/github/stars/microsoft/LMOps)](https://github.com/microsoft/LMOps/)

 > ğŸ‘¨â€ğŸ“ **Liang Wang** Â· ğŸ“§ **Furu Wei** Â· ğŸ›ï¸ **MSRA** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Llama-3.1-8B-Instruct Â· ğŸ¯ **Training**: REINFORCE, GRPO, PPO

[IKEA: Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent](https://arxiv.org/abs/2505.07596) [![[code]](https://img.shields.io/github/stars/hzy312/knowledge-r1)](https://github.com/hzy312/knowledge-r1)

 > ğŸ‘¨â€ğŸ“ **Ziyang Huang** Â· ğŸ“§ **Kang Liu** Â· ğŸ›ï¸ **IA, CAS** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Qwen-2.5-3B / 7B Â· ğŸ¯ **Training**: GRPO

[Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)

> ğŸ‘¨â€ğŸ“ **Hongjin Qian** Â· ğŸ“§ **Zheng Liu** Â· ğŸ›ï¸ **BAAI** \
> ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Qwen-2.5-3B / 7B Â· ğŸ¯ **Training**: GRPO, PPO


[Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/pdf/2505.11277) [![[code]](https://img.shields.io/github/stars/syr-cn/AutoRefine)](https://github.com/syr-cn/AutoRefine)

> ğŸ‘¨â€ğŸ“ **Yaorui Shi** Â· ğŸ“§ **Xiang Wang** Â· ğŸ›ï¸ **USTC** \
> ğŸ“Š **Dataset**: General QA, Multi-Hop QA Â· ğŸ¤– **Model**: Qwen-2.5-3B Â· ğŸ¯ **Training**: GRPO

[ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.15776) [![[code]](https://img.shields.io/github/stars/BeastyZ/ConvSearch-R1)](https://github.com/BeastyZ/ConvSearch-R1)

> ğŸ‘¨â€ğŸ“ **Changtai Zhu** Â· ğŸ“§ **Xipeng Qiu** Â· ğŸ›ï¸ **FDU** \
> ğŸ“Š **Dataset**: Conversational QA Â· ğŸ¤– **Model**: Qwen-2.5-3B / Llama-3.2-3B Â· ğŸ¯ **Training**: SFT, GRPO




### ğŸ”„ Workflow-based
[Search-o1: Agentic Search-Enhanced Large Reasoning Models](https://arxiv.org/abs/2501.05366):  [![[code]](https://img.shields.io/github/stars/sunnynexus/Search-o1?style=social)](https://github.com/sunnynexus/Search-o1)

 > ğŸ‘¨â€ğŸ“ **Xiaoxi Li** Â· ğŸ“§ **Zhicheng Dou** Â· ğŸ›ï¸ **GSAI, RUC** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA, Complex Task, Math & Coding Â· ğŸ¤– **Model**: QwQ-32B-Preview

 [Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research](https://arxiv.org/pdf/2502.04644) [![[code]](https://img.shields.io/github/stars/theworldofagents/Agentic-Reasoning)](https://github.com/theworldofagents/Agentic-Reasoning)

> ğŸ‘¨â€ğŸ“ **Junde Wu** Â· ğŸ“§ **Yuyuan Liu** Â· ğŸ›ï¸ **Oxford University** \
> ğŸ“Š **Dataset**: Complex Task Â· ğŸ¤– **Model**: APIs


###  ğŸ”§ Tool Using

[Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410) [![[code]](https://img.shields.io/github/stars/dongguanting/Tool-Star)](https://github.com/dongguanting/Tool-Star)

> ğŸ‘¨â€ğŸ“ **Guanting Dong** Â· ğŸ“§ **Zhicheng Dou** Â· ğŸ›ï¸ **GSAI, RUC** \
> ğŸ“Š **Dataset**: General QA, Multi-Hop QA, Math & Coding Â· ğŸ¤– **Model**: Qwen-2.5-3BÂ· ğŸ¯ **Training**: SFT,GRPO, PPO

[OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/pdf/2504.14870)

 > ğŸ‘¨â€ğŸ“ **Hongru Wang** Â· ğŸ“§ **Heng Ji** Â· ğŸ›ï¸ **CUHK** \
 > ğŸ“Š **Dataset**: General QA, Multi-Hop QA, Math & Coding Â· ğŸ¤– **Model**: Qwen-2.5-3B / 7BÂ· ğŸ¯ **Training**: GRPO, PPO

### ğŸ–¼ï¸ Multi-Modal
[Multimodal-Search-R1: Incentivizing LMMs to Search](https://kimingng.notion.site/MMSearch-R1-Incentivizing-LMMs-to-Search-1bcce992031880b2bc64fde13ef83e2a) [![[code]](https://img.shields.io/github/stars/EvolvingLMMs-Lab/multimodal-search-r1)](https://github.com/EvolvingLMMs-Lab/multimodal-search-r1)

 > ğŸ‘¨â€ğŸ“ **Jinming Wu** Â· ğŸ“§ **Zejun Ma** Â· ğŸ›ï¸ **BUPT** \
 > ğŸ“Š **Dataset**: VQA Â· ğŸ¤– **Model**: Qwen2.5-VL-Instruct-3B/7B Â· ğŸ¯ **Training**: GRPO

### ğŸ“Š Evaluation and Dataset

[InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.15872) [![[code]](https://img.shields.io/github/stars/YunjiaXi/InfoDeepSeek)](https://github.com/YunjiaXi/InfoDeepSeek)

> ğŸ‘¨â€ğŸ“ **Yunjia Xi** Â· ğŸ“§ **Jianghao Lin** Â· ğŸ›ï¸ **SJTU** \
> ğŸ“Š **Dataset**: General QA, Multi-Hop QA

[BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents](https://arxiv.org/abs/2504.12516) [![[code]](https://img.shields.io/github/stars/openai/simple-evals)](https://github.com/openai/simple-evals)

> ğŸ‘¨â€ğŸ“ **Jason Wei** Â· ğŸ“§ **Amelia Glaese** Â· ğŸ›ï¸ **OpenAI** \
> ğŸ“Š **Dataset**: Web Browsing


### ğŸ¢ Industry Solutions

OpenAI's  Deep Research: https://openai.com/index/introducing-deep-research/

Google's Gemini Pro: https://www.google.com/search/about/

X's Grok 3: https://x.ai/news/grok-3

Perplexity: https://www.perplexity.ai/

Jina AI: https://jina.ai/deepsearch/

Metasota: https://metaso.cn/

## ğŸ® Demo
We are building a demo page to showcase different agentic search methods and allow hands-on exploration of their capabilities. Each demo will be integrated into a standardized retrieval and web browser interface with comparable settings, enabling comprehensive and fair comparisons across various approaches. This systematic evaluation will help identify strengths and limitations of different methods and advance the state-of-the-art in agentic search.

## ğŸ“ Slides
We maintain a collection of ğŸ“Š paper presentation slides on Overleaf to facilitate learning and knowledge sharing in the agentic search community. Each presentation consists of 3-5 slides that concisely introduce key aspects of a paper, including motivation, methodology, and main results. These slides serve as quick references for understanding important works in the field and can be used for self-study, teaching, or research presentations.

ğŸ”— Check out our slides collection: [Agentic Search Paper Slides](https://www.overleaf.com/read/dhbksrdxswps#3990a3)

## ğŸ† Arena
We are building an arena page to benchmark different agentic search methods in a unified evaluation framework. All methods will be integrated into standardized retrieval and web browser interfaces with comparable settings, enabling comprehensive and fair comparisons across various approaches. This systematic evaluation will help identify strengths and limitations of different methods and advance the state-of-the-art in agentic search.

## ğŸ‹ï¸ Gym
We are organizing a collection of optimization frameworks and training approaches used in agentic search, including reinforcement learning methods like GRPO and PPO, as well as supervised fine-tuning techniques. This will help researchers understand and implement effective training strategies for their agentic search models.

Stay tuned for detailed tutorials and code examples on training agentic search systems!



## ğŸ¤ Contributing
We welcome contributions to this repository! If you have any suggestions or feedback, please feel free to open an issue or submit a pull request.


## ğŸ“– Citation
If you find this repository useful, please consider citing it as follows:
```bibtex
@misc{awesome-agentic-search,
  author = {Hongjin Qian, Zheng Liu},
  title = {Awesome Agentic Search},
  year = {2025},
  publisher = {GitHub},
```